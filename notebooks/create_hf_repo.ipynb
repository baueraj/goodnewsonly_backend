{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pr huggingface-cli login  # but I didn't use poetry run alias in immediately below command...\n",
    "# huggingface-cli repo create distilbert-base-uncased-finetuned-sst-2-eng-sentiment-int8 --type model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:55:11.734140643Z",
     "start_time": "2024-01-19T02:55:09.936904662Z"
    }
   },
   "id": "dc4492b0900877d4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "quantized_model_path = \"../goodnewsonly/resources/quant_model_weights_and_arch.pth.gz\"\n",
    "hf_repo_dir = \"/home/abauer/other_projects/goodnewsonly/distilbert-base-uncased-finetuned-sst-2-eng-sentiment-int8\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:55:13.665730598Z",
     "start_time": "2024-01-19T02:55:13.649460238Z"
    }
   },
   "id": "7b1f956e1ff9d2f4",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abauer/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/torch/_utils.py:355: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(quantized_model_path, \"rb\") as f:\n",
    "    buffer = f.read()\n",
    "quantized_model = torch.load(io.BytesIO(buffer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:55:19.815869065Z",
     "start_time": "2024-01-19T02:55:18.705949751Z"
    }
   },
   "id": "5f6b352f92953d98",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2edadf4bb5e2492ca205cf92c36dd1a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:55:30.620412436Z",
     "start_time": "2024-01-19T02:55:21.298257795Z"
    }
   },
   "id": "d1740056003048fb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DistilBertForSequenceClassification:\n\tMissing key(s) in state_dict: \"distilbert.transformer.layer.0.attention.q_lin.weight\", \"distilbert.transformer.layer.0.attention.q_lin.bias\", \"distilbert.transformer.layer.0.attention.k_lin.weight\", \"distilbert.transformer.layer.0.attention.k_lin.bias\", \"distilbert.transformer.layer.0.attention.v_lin.weight\", \"distilbert.transformer.layer.0.attention.v_lin.bias\", \"distilbert.transformer.layer.0.attention.out_lin.weight\", \"distilbert.transformer.layer.0.attention.out_lin.bias\", \"distilbert.transformer.layer.0.ffn.lin1.weight\", \"distilbert.transformer.layer.0.ffn.lin1.bias\", \"distilbert.transformer.layer.0.ffn.lin2.weight\", \"distilbert.transformer.layer.0.ffn.lin2.bias\", \"distilbert.transformer.layer.1.attention.q_lin.weight\", \"distilbert.transformer.layer.1.attention.q_lin.bias\", \"distilbert.transformer.layer.1.attention.k_lin.weight\", \"distilbert.transformer.layer.1.attention.k_lin.bias\", \"distilbert.transformer.layer.1.attention.v_lin.weight\", \"distilbert.transformer.layer.1.attention.v_lin.bias\", \"distilbert.transformer.layer.1.attention.out_lin.weight\", \"distilbert.transformer.layer.1.attention.out_lin.bias\", \"distilbert.transformer.layer.1.ffn.lin1.weight\", \"distilbert.transformer.layer.1.ffn.lin1.bias\", \"distilbert.transformer.layer.1.ffn.lin2.weight\", \"distilbert.transformer.layer.1.ffn.lin2.bias\", \"distilbert.transformer.layer.2.attention.q_lin.weight\", \"distilbert.transformer.layer.2.attention.q_lin.bias\", \"distilbert.transformer.layer.2.attention.k_lin.weight\", \"distilbert.transformer.layer.2.attention.k_lin.bias\", \"distilbert.transformer.layer.2.attention.v_lin.weight\", \"distilbert.transformer.layer.2.attention.v_lin.bias\", \"distilbert.transformer.layer.2.attention.out_lin.weight\", \"distilbert.transformer.layer.2.attention.out_lin.bias\", \"distilbert.transformer.layer.2.ffn.lin1.weight\", \"distilbert.transformer.layer.2.ffn.lin1.bias\", \"distilbert.transformer.layer.2.ffn.lin2.weight\", \"distilbert.transformer.layer.2.ffn.lin2.bias\", \"distilbert.transformer.layer.3.attention.q_lin.weight\", \"distilbert.transformer.layer.3.attention.q_lin.bias\", \"distilbert.transformer.layer.3.attention.k_lin.weight\", \"distilbert.transformer.layer.3.attention.k_lin.bias\", \"distilbert.transformer.layer.3.attention.v_lin.weight\", \"distilbert.transformer.layer.3.attention.v_lin.bias\", \"distilbert.transformer.layer.3.attention.out_lin.weight\", \"distilbert.transformer.layer.3.attention.out_lin.bias\", \"distilbert.transformer.layer.3.ffn.lin1.weight\", \"distilbert.transformer.layer.3.ffn.lin1.bias\", \"distilbert.transformer.layer.3.ffn.lin2.weight\", \"distilbert.transformer.layer.3.ffn.lin2.bias\", \"distilbert.transformer.layer.4.attention.q_lin.weight\", \"distilbert.transformer.layer.4.attention.q_lin.bias\", \"distilbert.transformer.layer.4.attention.k_lin.weight\", \"distilbert.transformer.layer.4.attention.k_lin.bias\", \"distilbert.transformer.layer.4.attention.v_lin.weight\", \"distilbert.transformer.layer.4.attention.v_lin.bias\", \"distilbert.transformer.layer.4.attention.out_lin.weight\", \"distilbert.transformer.layer.4.attention.out_lin.bias\", \"distilbert.transformer.layer.4.ffn.lin1.weight\", \"distilbert.transformer.layer.4.ffn.lin1.bias\", \"distilbert.transformer.layer.4.ffn.lin2.weight\", \"distilbert.transformer.layer.4.ffn.lin2.bias\", \"distilbert.transformer.layer.5.attention.q_lin.weight\", \"distilbert.transformer.layer.5.attention.q_lin.bias\", \"distilbert.transformer.layer.5.attention.k_lin.weight\", \"distilbert.transformer.layer.5.attention.k_lin.bias\", \"distilbert.transformer.layer.5.attention.v_lin.weight\", \"distilbert.transformer.layer.5.attention.v_lin.bias\", \"distilbert.transformer.layer.5.attention.out_lin.weight\", \"distilbert.transformer.layer.5.attention.out_lin.bias\", \"distilbert.transformer.layer.5.ffn.lin1.weight\", \"distilbert.transformer.layer.5.ffn.lin1.bias\", \"distilbert.transformer.layer.5.ffn.lin2.weight\", \"distilbert.transformer.layer.5.ffn.lin2.bias\", \"pre_classifier.weight\", \"pre_classifier.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"distilbert.transformer.layer.0.attention.q_lin.scale\", \"distilbert.transformer.layer.0.attention.q_lin.zero_point\", \"distilbert.transformer.layer.0.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.k_lin.scale\", \"distilbert.transformer.layer.0.attention.k_lin.zero_point\", \"distilbert.transformer.layer.0.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.v_lin.scale\", \"distilbert.transformer.layer.0.attention.v_lin.zero_point\", \"distilbert.transformer.layer.0.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.out_lin.scale\", \"distilbert.transformer.layer.0.attention.out_lin.zero_point\", \"distilbert.transformer.layer.0.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.ffn.lin1.scale\", \"distilbert.transformer.layer.0.ffn.lin1.zero_point\", \"distilbert.transformer.layer.0.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.0.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.0.ffn.lin2.scale\", \"distilbert.transformer.layer.0.ffn.lin2.zero_point\", \"distilbert.transformer.layer.0.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.0.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.q_lin.scale\", \"distilbert.transformer.layer.1.attention.q_lin.zero_point\", \"distilbert.transformer.layer.1.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.k_lin.scale\", \"distilbert.transformer.layer.1.attention.k_lin.zero_point\", \"distilbert.transformer.layer.1.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.v_lin.scale\", \"distilbert.transformer.layer.1.attention.v_lin.zero_point\", \"distilbert.transformer.layer.1.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.out_lin.scale\", \"distilbert.transformer.layer.1.attention.out_lin.zero_point\", \"distilbert.transformer.layer.1.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.ffn.lin1.scale\", \"distilbert.transformer.layer.1.ffn.lin1.zero_point\", \"distilbert.transformer.layer.1.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.1.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.1.ffn.lin2.scale\", \"distilbert.transformer.layer.1.ffn.lin2.zero_point\", \"distilbert.transformer.layer.1.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.1.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.q_lin.scale\", \"distilbert.transformer.layer.2.attention.q_lin.zero_point\", \"distilbert.transformer.layer.2.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.k_lin.scale\", \"distilbert.transformer.layer.2.attention.k_lin.zero_point\", \"distilbert.transformer.layer.2.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.v_lin.scale\", \"distilbert.transformer.layer.2.attention.v_lin.zero_point\", \"distilbert.transformer.layer.2.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.out_lin.scale\", \"distilbert.transformer.layer.2.attention.out_lin.zero_point\", \"distilbert.transformer.layer.2.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.ffn.lin1.scale\", \"distilbert.transformer.layer.2.ffn.lin1.zero_point\", \"distilbert.transformer.layer.2.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.2.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.2.ffn.lin2.scale\", \"distilbert.transformer.layer.2.ffn.lin2.zero_point\", \"distilbert.transformer.layer.2.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.2.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.q_lin.scale\", \"distilbert.transformer.layer.3.attention.q_lin.zero_point\", \"distilbert.transformer.layer.3.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.k_lin.scale\", \"distilbert.transformer.layer.3.attention.k_lin.zero_point\", \"distilbert.transformer.layer.3.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.v_lin.scale\", \"distilbert.transformer.layer.3.attention.v_lin.zero_point\", \"distilbert.transformer.layer.3.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.out_lin.scale\", \"distilbert.transformer.layer.3.attention.out_lin.zero_point\", \"distilbert.transformer.layer.3.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.ffn.lin1.scale\", \"distilbert.transformer.layer.3.ffn.lin1.zero_point\", \"distilbert.transformer.layer.3.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.3.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.3.ffn.lin2.scale\", \"distilbert.transformer.layer.3.ffn.lin2.zero_point\", \"distilbert.transformer.layer.3.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.3.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.q_lin.scale\", \"distilbert.transformer.layer.4.attention.q_lin.zero_point\", \"distilbert.transformer.layer.4.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.k_lin.scale\", \"distilbert.transformer.layer.4.attention.k_lin.zero_point\", \"distilbert.transformer.layer.4.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.v_lin.scale\", \"distilbert.transformer.layer.4.attention.v_lin.zero_point\", \"distilbert.transformer.layer.4.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.out_lin.scale\", \"distilbert.transformer.layer.4.attention.out_lin.zero_point\", \"distilbert.transformer.layer.4.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.ffn.lin1.scale\", \"distilbert.transformer.layer.4.ffn.lin1.zero_point\", \"distilbert.transformer.layer.4.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.4.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.4.ffn.lin2.scale\", \"distilbert.transformer.layer.4.ffn.lin2.zero_point\", \"distilbert.transformer.layer.4.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.4.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.q_lin.scale\", \"distilbert.transformer.layer.5.attention.q_lin.zero_point\", \"distilbert.transformer.layer.5.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.k_lin.scale\", \"distilbert.transformer.layer.5.attention.k_lin.zero_point\", \"distilbert.transformer.layer.5.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.v_lin.scale\", \"distilbert.transformer.layer.5.attention.v_lin.zero_point\", \"distilbert.transformer.layer.5.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.out_lin.scale\", \"distilbert.transformer.layer.5.attention.out_lin.zero_point\", \"distilbert.transformer.layer.5.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.ffn.lin1.scale\", \"distilbert.transformer.layer.5.ffn.lin1.zero_point\", \"distilbert.transformer.layer.5.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.5.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.5.ffn.lin2.scale\", \"distilbert.transformer.layer.5.ffn.lin2.zero_point\", \"distilbert.transformer.layer.5.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.5.ffn.lin2._packed_params._packed_params\", \"pre_classifier.scale\", \"pre_classifier.zero_point\", \"pre_classifier._packed_params.dtype\", \"pre_classifier._packed_params._packed_params\", \"classifier.scale\", \"classifier.zero_point\", \"classifier._packed_params.dtype\", \"classifier._packed_params._packed_params\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43morig_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquantized_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:2152\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2147\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2148\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2149\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2152\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2153\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for DistilBertForSequenceClassification:\n\tMissing key(s) in state_dict: \"distilbert.transformer.layer.0.attention.q_lin.weight\", \"distilbert.transformer.layer.0.attention.q_lin.bias\", \"distilbert.transformer.layer.0.attention.k_lin.weight\", \"distilbert.transformer.layer.0.attention.k_lin.bias\", \"distilbert.transformer.layer.0.attention.v_lin.weight\", \"distilbert.transformer.layer.0.attention.v_lin.bias\", \"distilbert.transformer.layer.0.attention.out_lin.weight\", \"distilbert.transformer.layer.0.attention.out_lin.bias\", \"distilbert.transformer.layer.0.ffn.lin1.weight\", \"distilbert.transformer.layer.0.ffn.lin1.bias\", \"distilbert.transformer.layer.0.ffn.lin2.weight\", \"distilbert.transformer.layer.0.ffn.lin2.bias\", \"distilbert.transformer.layer.1.attention.q_lin.weight\", \"distilbert.transformer.layer.1.attention.q_lin.bias\", \"distilbert.transformer.layer.1.attention.k_lin.weight\", \"distilbert.transformer.layer.1.attention.k_lin.bias\", \"distilbert.transformer.layer.1.attention.v_lin.weight\", \"distilbert.transformer.layer.1.attention.v_lin.bias\", \"distilbert.transformer.layer.1.attention.out_lin.weight\", \"distilbert.transformer.layer.1.attention.out_lin.bias\", \"distilbert.transformer.layer.1.ffn.lin1.weight\", \"distilbert.transformer.layer.1.ffn.lin1.bias\", \"distilbert.transformer.layer.1.ffn.lin2.weight\", \"distilbert.transformer.layer.1.ffn.lin2.bias\", \"distilbert.transformer.layer.2.attention.q_lin.weight\", \"distilbert.transformer.layer.2.attention.q_lin.bias\", \"distilbert.transformer.layer.2.attention.k_lin.weight\", \"distilbert.transformer.layer.2.attention.k_lin.bias\", \"distilbert.transformer.layer.2.attention.v_lin.weight\", \"distilbert.transformer.layer.2.attention.v_lin.bias\", \"distilbert.transformer.layer.2.attention.out_lin.weight\", \"distilbert.transformer.layer.2.attention.out_lin.bias\", \"distilbert.transformer.layer.2.ffn.lin1.weight\", \"distilbert.transformer.layer.2.ffn.lin1.bias\", \"distilbert.transformer.layer.2.ffn.lin2.weight\", \"distilbert.transformer.layer.2.ffn.lin2.bias\", \"distilbert.transformer.layer.3.attention.q_lin.weight\", \"distilbert.transformer.layer.3.attention.q_lin.bias\", \"distilbert.transformer.layer.3.attention.k_lin.weight\", \"distilbert.transformer.layer.3.attention.k_lin.bias\", \"distilbert.transformer.layer.3.attention.v_lin.weight\", \"distilbert.transformer.layer.3.attention.v_lin.bias\", \"distilbert.transformer.layer.3.attention.out_lin.weight\", \"distilbert.transformer.layer.3.attention.out_lin.bias\", \"distilbert.transformer.layer.3.ffn.lin1.weight\", \"distilbert.transformer.layer.3.ffn.lin1.bias\", \"distilbert.transformer.layer.3.ffn.lin2.weight\", \"distilbert.transformer.layer.3.ffn.lin2.bias\", \"distilbert.transformer.layer.4.attention.q_lin.weight\", \"distilbert.transformer.layer.4.attention.q_lin.bias\", \"distilbert.transformer.layer.4.attention.k_lin.weight\", \"distilbert.transformer.layer.4.attention.k_lin.bias\", \"distilbert.transformer.layer.4.attention.v_lin.weight\", \"distilbert.transformer.layer.4.attention.v_lin.bias\", \"distilbert.transformer.layer.4.attention.out_lin.weight\", \"distilbert.transformer.layer.4.attention.out_lin.bias\", \"distilbert.transformer.layer.4.ffn.lin1.weight\", \"distilbert.transformer.layer.4.ffn.lin1.bias\", \"distilbert.transformer.layer.4.ffn.lin2.weight\", \"distilbert.transformer.layer.4.ffn.lin2.bias\", \"distilbert.transformer.layer.5.attention.q_lin.weight\", \"distilbert.transformer.layer.5.attention.q_lin.bias\", \"distilbert.transformer.layer.5.attention.k_lin.weight\", \"distilbert.transformer.layer.5.attention.k_lin.bias\", \"distilbert.transformer.layer.5.attention.v_lin.weight\", \"distilbert.transformer.layer.5.attention.v_lin.bias\", \"distilbert.transformer.layer.5.attention.out_lin.weight\", \"distilbert.transformer.layer.5.attention.out_lin.bias\", \"distilbert.transformer.layer.5.ffn.lin1.weight\", \"distilbert.transformer.layer.5.ffn.lin1.bias\", \"distilbert.transformer.layer.5.ffn.lin2.weight\", \"distilbert.transformer.layer.5.ffn.lin2.bias\", \"pre_classifier.weight\", \"pre_classifier.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"distilbert.transformer.layer.0.attention.q_lin.scale\", \"distilbert.transformer.layer.0.attention.q_lin.zero_point\", \"distilbert.transformer.layer.0.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.k_lin.scale\", \"distilbert.transformer.layer.0.attention.k_lin.zero_point\", \"distilbert.transformer.layer.0.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.v_lin.scale\", \"distilbert.transformer.layer.0.attention.v_lin.zero_point\", \"distilbert.transformer.layer.0.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.attention.out_lin.scale\", \"distilbert.transformer.layer.0.attention.out_lin.zero_point\", \"distilbert.transformer.layer.0.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.0.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.0.ffn.lin1.scale\", \"distilbert.transformer.layer.0.ffn.lin1.zero_point\", \"distilbert.transformer.layer.0.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.0.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.0.ffn.lin2.scale\", \"distilbert.transformer.layer.0.ffn.lin2.zero_point\", \"distilbert.transformer.layer.0.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.0.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.q_lin.scale\", \"distilbert.transformer.layer.1.attention.q_lin.zero_point\", \"distilbert.transformer.layer.1.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.k_lin.scale\", \"distilbert.transformer.layer.1.attention.k_lin.zero_point\", \"distilbert.transformer.layer.1.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.v_lin.scale\", \"distilbert.transformer.layer.1.attention.v_lin.zero_point\", \"distilbert.transformer.layer.1.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.attention.out_lin.scale\", \"distilbert.transformer.layer.1.attention.out_lin.zero_point\", \"distilbert.transformer.layer.1.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.1.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.1.ffn.lin1.scale\", \"distilbert.transformer.layer.1.ffn.lin1.zero_point\", \"distilbert.transformer.layer.1.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.1.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.1.ffn.lin2.scale\", \"distilbert.transformer.layer.1.ffn.lin2.zero_point\", \"distilbert.transformer.layer.1.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.1.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.q_lin.scale\", \"distilbert.transformer.layer.2.attention.q_lin.zero_point\", \"distilbert.transformer.layer.2.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.k_lin.scale\", \"distilbert.transformer.layer.2.attention.k_lin.zero_point\", \"distilbert.transformer.layer.2.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.v_lin.scale\", \"distilbert.transformer.layer.2.attention.v_lin.zero_point\", \"distilbert.transformer.layer.2.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.attention.out_lin.scale\", \"distilbert.transformer.layer.2.attention.out_lin.zero_point\", \"distilbert.transformer.layer.2.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.2.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.2.ffn.lin1.scale\", \"distilbert.transformer.layer.2.ffn.lin1.zero_point\", \"distilbert.transformer.layer.2.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.2.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.2.ffn.lin2.scale\", \"distilbert.transformer.layer.2.ffn.lin2.zero_point\", \"distilbert.transformer.layer.2.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.2.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.q_lin.scale\", \"distilbert.transformer.layer.3.attention.q_lin.zero_point\", \"distilbert.transformer.layer.3.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.k_lin.scale\", \"distilbert.transformer.layer.3.attention.k_lin.zero_point\", \"distilbert.transformer.layer.3.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.v_lin.scale\", \"distilbert.transformer.layer.3.attention.v_lin.zero_point\", \"distilbert.transformer.layer.3.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.attention.out_lin.scale\", \"distilbert.transformer.layer.3.attention.out_lin.zero_point\", \"distilbert.transformer.layer.3.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.3.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.3.ffn.lin1.scale\", \"distilbert.transformer.layer.3.ffn.lin1.zero_point\", \"distilbert.transformer.layer.3.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.3.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.3.ffn.lin2.scale\", \"distilbert.transformer.layer.3.ffn.lin2.zero_point\", \"distilbert.transformer.layer.3.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.3.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.q_lin.scale\", \"distilbert.transformer.layer.4.attention.q_lin.zero_point\", \"distilbert.transformer.layer.4.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.k_lin.scale\", \"distilbert.transformer.layer.4.attention.k_lin.zero_point\", \"distilbert.transformer.layer.4.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.v_lin.scale\", \"distilbert.transformer.layer.4.attention.v_lin.zero_point\", \"distilbert.transformer.layer.4.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.attention.out_lin.scale\", \"distilbert.transformer.layer.4.attention.out_lin.zero_point\", \"distilbert.transformer.layer.4.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.4.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.4.ffn.lin1.scale\", \"distilbert.transformer.layer.4.ffn.lin1.zero_point\", \"distilbert.transformer.layer.4.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.4.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.4.ffn.lin2.scale\", \"distilbert.transformer.layer.4.ffn.lin2.zero_point\", \"distilbert.transformer.layer.4.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.4.ffn.lin2._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.q_lin.scale\", \"distilbert.transformer.layer.5.attention.q_lin.zero_point\", \"distilbert.transformer.layer.5.attention.q_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.q_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.k_lin.scale\", \"distilbert.transformer.layer.5.attention.k_lin.zero_point\", \"distilbert.transformer.layer.5.attention.k_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.k_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.v_lin.scale\", \"distilbert.transformer.layer.5.attention.v_lin.zero_point\", \"distilbert.transformer.layer.5.attention.v_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.v_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.attention.out_lin.scale\", \"distilbert.transformer.layer.5.attention.out_lin.zero_point\", \"distilbert.transformer.layer.5.attention.out_lin._packed_params.dtype\", \"distilbert.transformer.layer.5.attention.out_lin._packed_params._packed_params\", \"distilbert.transformer.layer.5.ffn.lin1.scale\", \"distilbert.transformer.layer.5.ffn.lin1.zero_point\", \"distilbert.transformer.layer.5.ffn.lin1._packed_params.dtype\", \"distilbert.transformer.layer.5.ffn.lin1._packed_params._packed_params\", \"distilbert.transformer.layer.5.ffn.lin2.scale\", \"distilbert.transformer.layer.5.ffn.lin2.zero_point\", \"distilbert.transformer.layer.5.ffn.lin2._packed_params.dtype\", \"distilbert.transformer.layer.5.ffn.lin2._packed_params._packed_params\", \"pre_classifier.scale\", \"pre_classifier.zero_point\", \"pre_classifier._packed_params.dtype\", \"pre_classifier._packed_params._packed_params\", \"classifier.scale\", \"classifier.zero_point\", \"classifier._packed_params.dtype\", \"classifier._packed_params._packed_params\". "
     ]
    }
   ],
   "source": [
    "orig_model.load_state_dict(quantized_model.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:55:34.384770318Z",
     "start_time": "2024-01-19T02:55:33.817804188Z"
    }
   },
   "id": "e4d89c6b658bca30",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'distilbert.transformer.layer.5.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.5.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.5.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.1.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.4.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.1.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.3.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin2._packed_params.dtype', 'classifier._packed_params.dtype', 'distilbert.transformer.layer.5.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.5.attention.v_lin._packed_params.dtype', 'pre_classifier._packed_params.dtype', 'distilbert.transformer.layer.1.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.1.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.1.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.1.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.5.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.out_lin._packed_params.dtype'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Save the model using Hugging Face's method\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhf_repo_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:2349\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   2346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2347\u001B[0m     weights_name \u001B[38;5;241m=\u001B[39m ADAPTER_SAFE_WEIGHTS_NAME \u001B[38;5;28;01mif\u001B[39;00m safe_serialization \u001B[38;5;28;01melse\u001B[39;00m ADAPTER_WEIGHTS_NAME\n\u001B[0;32m-> 2349\u001B[0m shards, index \u001B[38;5;241m=\u001B[39m \u001B[43mshard_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_shard_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_shard_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2351\u001B[0m \u001B[38;5;66;03m# Clean the folder from a previous save\u001B[39;00m\n\u001B[1;32m   2352\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(save_directory):\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:370\u001B[0m, in \u001B[0;36mshard_checkpoint\u001B[0;34m(state_dict, max_shard_size, weights_name)\u001B[0m\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 370\u001B[0m     storage_id \u001B[38;5;241m=\u001B[39m \u001B[43mid_tensor_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;66;03m# If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m storage_id \u001B[38;5;129;01min\u001B[39;00m storage_id_to_block:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/pytorch_utils.py:290\u001B[0m, in \u001B[0;36mid_tensor_storage\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mid_tensor_storage\u001B[39m(tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m    284\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    Unique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001B[39;00m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;124;03m    example, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;124;03m    guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;124;03m    non-overlapping lifetimes may have the same id.\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 290\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_tpu_available():\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;66;03m# NOTE: xla tensors dont have storage\u001B[39;00m\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;66;03m# use some other unique id to distinguish.\u001B[39;00m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;66;03m# this is a XLA tensor, it must be created using torch_xla's\u001B[39;00m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;66;03m# device. So the following import is safe:\u001B[39;00m\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_xla\u001B[39;00m\n\u001B[1;32m    297\u001B[0m         unique_id \u001B[38;5;241m=\u001B[39m torch_xla\u001B[38;5;241m.\u001B[39m_XLAC\u001B[38;5;241m.\u001B[39m_xla_get_tensor_id(tensor)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'torch.dtype' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# Save the model using Hugging Face's method\n",
    "orig_model.save_pretrained(hf_repo_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:42:53.727792543Z",
     "start_time": "2024-01-19T02:42:53.005278128Z"
    }
   },
   "id": "81fba6f76836263b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9952838186bb44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
