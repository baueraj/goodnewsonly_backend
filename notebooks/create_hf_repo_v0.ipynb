{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pr huggingface-cli login  # but I didn't use poetry run alias in immediately below command...\n",
    "# huggingface-cli repo create distilbert-base-uncased-finetuned-sst-2-eng-sentiment-int8 --type model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import torch\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:41:30.978320854Z",
     "start_time": "2024-01-19T02:41:28.928718672Z"
    }
   },
   "id": "dc4492b0900877d4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = \"../goodnewsonly/resources/quant_model_weights_and_arch.pth.gz\"\n",
    "hf_repo_dir = \"/home/abauer/other_projects/goodnewsonly/distilbert-base-uncased-finetuned-sst-2-eng-sentiment-int8\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:41:30.984331937Z",
     "start_time": "2024-01-19T02:41:30.980410691Z"
    }
   },
   "id": "7b1f956e1ff9d2f4",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abauer/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/torch/_utils.py:355: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(model_path, \"rb\") as f:\n",
    "    buffer = f.read()\n",
    "model = torch.load(io.BytesIO(buffer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:41:36.985945266Z",
     "start_time": "2024-01-19T02:41:35.696699025Z"
    }
   },
   "id": "5f6b352f92953d98",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:41:53.357345160Z",
     "start_time": "2024-01-19T02:41:53.203515428Z"
    }
   },
   "id": "d1740056003048fb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'distilbert.transformer.layer.5.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.5.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.5.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.1.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.4.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.1.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.3.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.0.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.ffn.lin2._packed_params.dtype', 'classifier._packed_params.dtype', 'distilbert.transformer.layer.5.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.5.attention.v_lin._packed_params.dtype', 'pre_classifier._packed_params.dtype', 'distilbert.transformer.layer.1.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.1.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.0.ffn.lin1._packed_params.dtype', 'distilbert.transformer.layer.2.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.2.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.4.attention.k_lin._packed_params.dtype', 'distilbert.transformer.layer.1.attention.out_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.4.ffn.lin2._packed_params.dtype', 'distilbert.transformer.layer.1.attention.v_lin._packed_params.dtype', 'distilbert.transformer.layer.5.attention.q_lin._packed_params.dtype', 'distilbert.transformer.layer.3.attention.out_lin._packed_params.dtype'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Save the model using Hugging Face's method\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhf_repo_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:2349\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   2346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2347\u001B[0m     weights_name \u001B[38;5;241m=\u001B[39m ADAPTER_SAFE_WEIGHTS_NAME \u001B[38;5;28;01mif\u001B[39;00m safe_serialization \u001B[38;5;28;01melse\u001B[39;00m ADAPTER_WEIGHTS_NAME\n\u001B[0;32m-> 2349\u001B[0m shards, index \u001B[38;5;241m=\u001B[39m \u001B[43mshard_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_shard_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_shard_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2351\u001B[0m \u001B[38;5;66;03m# Clean the folder from a previous save\u001B[39;00m\n\u001B[1;32m   2352\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(save_directory):\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:370\u001B[0m, in \u001B[0;36mshard_checkpoint\u001B[0;34m(state_dict, max_shard_size, weights_name)\u001B[0m\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 370\u001B[0m     storage_id \u001B[38;5;241m=\u001B[39m \u001B[43mid_tensor_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;66;03m# If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m storage_id \u001B[38;5;129;01min\u001B[39;00m storage_id_to_block:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/goodnewsonly-8-cVQIQA-py3.9/lib/python3.9/site-packages/transformers/pytorch_utils.py:290\u001B[0m, in \u001B[0;36mid_tensor_storage\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mid_tensor_storage\u001B[39m(tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m    284\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    Unique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001B[39;00m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;124;03m    example, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;124;03m    guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;124;03m    non-overlapping lifetimes may have the same id.\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 290\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_tpu_available():\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;66;03m# NOTE: xla tensors dont have storage\u001B[39;00m\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;66;03m# use some other unique id to distinguish.\u001B[39;00m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;66;03m# this is a XLA tensor, it must be created using torch_xla's\u001B[39;00m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;66;03m# device. So the following import is safe:\u001B[39;00m\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_xla\u001B[39;00m\n\u001B[1;32m    297\u001B[0m         unique_id \u001B[38;5;241m=\u001B[39m torch_xla\u001B[38;5;241m.\u001B[39m_XLAC\u001B[38;5;241m.\u001B[39m_xla_get_tensor_id(tensor)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'torch.dtype' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# Save the model using Hugging Face's method\n",
    "model.save_pretrained(hf_repo_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T02:42:53.727792543Z",
     "start_time": "2024-01-19T02:42:53.005278128Z"
    }
   },
   "id": "81fba6f76836263b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9952838186bb44"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
